# MapReduce：简化了大集群上的数据处理

## 摘要

> MapReduce 是一种编程模型，一种处理并且产生大量数据集合的相关实现。用户指定一种 *map* 函数， *map* 函数可以对 *key/value* 进行处理， 产生中间状态的  *key/value* ，指定一种 *reduce* 函数， *reduce* 函数可以将所有中间状态的  *key/value*  通过相同的中间状态的 *key* 合并起来。在这篇论文中， 可以发现好多现实世界的任务都可以通过这种模型展现出来。
>
> 这种函数式的程序自动地在一个由大量商业机器组成的大集群上并行运行着。 运行着的系统负责数据的分区， 在这些机器集群上程序执行的调度，机器容错， 管理机器内部通信。这样允许没有任何并行和分布式系统经验的程序员可以方便的使用分布式集群的资源。
>
> 我们 MapReduce 的实现，是运行在一个由大量商业机器上的集群，并且是高度可扩展的：一个典型的MapReduce 在成千上万的机上面处理以 T 为单位计算的数据集合。程序员可以发现这个系统使用非常容易：实现了MapReduce的函数有成百上千个，而且每天有超过1000个MapReduce的任务在Google的集群上运行。

## 1.介绍（Introduction）

在过去的5年中，作者还有许多Google的其他同事为了处理海量的原始数据， 比如爬取的文档，web请求日志等等， 为了计算各种各样的衍生的数据，实现了成百上千个特定的计算程序， 比如倒排索引， 各种各样的网络文档的图数据结构，每个host上面爬取的页面汇总， 指定某天最频繁的请求的集合等等。大部分这些计算程序从概念上看都比较直接。 但是， 数据输入通常比较大，计算任务需要被分配到成百上千台机器上，并且需要在一个合理的时间内完成。如何处理并行计算、如何分发数据、如何处理错误？所有这些问题综合在一起，需要大量的代码处理，因此也使得原本简单的运算变得难以处理。

为了对付这种复杂度， 我们设计了一个新的抽象， 这种抽象模型只需要我们表述简单的计算，而将并行化处理， 容错， 数据分发，负载均衡封装到一个库里面。设计这种抽象的灵感来自来自 *Lisp* 还有其他函数式编程语言的 *map* 和 *reduce* 源语。我们实现了：对大部分相关的计算，应用一个 *map* 函数计算，将我们输入中的逻辑记录变成 *key/value* 对的集合， 然后应用一个 *reduce* 函数将所有具有相同 *key* 的 values 合理合并。 我们应用的这种指定*map reduce*的函数式编程模型允许我们方便的实现并行化计算， 并且增加重试（re-execution） 作为容灾的主要机制

这项工作的主要贡献是通过简单的接口来实现自动的并行化和大规模计算，并且通过实现了这种接口（map reduce）的程序， 可以实现在海量普通PC上的高性能计算。

第二部分描述基本的编程模型和一些使用案例。第三部分描述了一个经过裁剪的、适合我们的基于集群的计算环境的MapReduce实现。第四部分描述我们认为在MapReduce编程模型中一些实用的技巧。第五部分对于各种不同的任务，测量我们MapReduce实现的性能。第六部分揭示了在Google内部如何使用MapReduce作为基础重写我们的索引系统产品，包括其它一些使用MapReduce的经验。第七部分讨论相关的和未来的工作。

## 2. 编程模型（Programming Model）

这种计算使用了一组 *key/value* 对形式的输入，然后产生一组 *key/value* 对形式的输出。使用了 MapReduce 库的用户将计算表述成两种函数：*Map and Reduce*。

用户编写的*Map* 利用输入对（pair） 的形式并产生一组 中间状态的 *key/value* 对形式的数据。MapReduce 库将所有values以相同的中间状态的 key  （比如 *I*）分组合并然后将这些数据传递给 *Reduce* 函数

用户编写的*Reduce* 接受一个中间状态的 key （比如 *I*）和 与这 key 相关的一组values。这个函数将这些数据合并，尽可能形成一个小规模的values 集合。通常每个*Reduce* 调用只会产生 0 或者 1 个 输出值。用户可以迭代使用reduce函数生成一中间状态的values。 这也使得我们可以处理无法全部放入内存的那些数据

### 2.1 示例（Examples）

设想一个场景：计算大量文档中每个单词出现的数量。用户可能写出下面类似的伪代码

```java
map(String key, String value):
    // key: document name
    // value: document contents
    for each word w in value:
        EmitIntermediate(w, "1");

reduce(String key, Iterator values):
    // key: a word
    // values: a list of counts
    int result = 0;
    for each v in values:
    	result += ParseInt(v);
    Emit(AsString(result));
```

map 函数将每个单词加上相关的出现次数（这里是 1）。reduce 函数计算每个单词的汇总数量。

另外，用户需要在 mapreduce 相关的配置中写上输入输出的文件名，以及一些可选参数。然后用户可以使用这些参数调用 *MapReduce* 函数。用户编写的代码与MapReduce 库（C++实现）相连接。附录A包含这个示例的完整代码片段。

### 2.2 类型（Types）

尽管之前的伪代码是以 string 类型作为输入输出， 但是从概念上讲， 用户提供给map 和 reduce 函数的参数类型是如下定义

```text
map (k1,v1) → list(k2,v2)
reduce (k2,list(v2)) → list(v2)
```

定义：输入的keys和values 和 输出的keys，values 是来自不同的领域。进一步讲，中间状态的keys 和 values 和输出是来自同一领域。

我们的C++实现将 字符串数据通过我们定义的函数传递， 然后将这些数据丢给我们的代码， 来进行字符串和合适类型的相关转换。

### 2.3 更多示例（More Examples）

下面是几个比较有趣的可以用*MapReduce* 来计算的简单示例

**分布式检索（Distributed Grep）**：map 函数输出符合指定的模式（pattern）的一行文本。reduce 函数将所有中间状态的数据拷贝到输出

**计算URL的访问频率（Count Of URL Access Frequency）**: map 函数处理网页请求日志，然后以<URL, 1>这种形式输出。 reduce 函数将这些以相同URL 的数据累加，然后输出 <URL, total count> 键值对

**反转Web引用拓扑图（Reverse Web-Link Graph）**： map函数会将在一个网页（命名为source）上面发现的每个引用 target URL 以 <target, source> 形式输出。 然后reduce 函数将这些以相同target关联的source 合并为列表， 并输出<target, list(source)> 键值对

**每个主机上面的词条向量统计（Term-Vector per Host）**: 词向量（term vector）是对一个或者一系列文档上面出现的最重要的词语，以列表<word, frequency> 形式对输出。map 函数为每一个输入的文档输出一个<hostname, term vector>的键值对（其中host那么是从文档中的URL解析而来）。reduce 函数会接收这些数据， 将这些词向量累加， 丢弃掉低频词汇，然后最终输出<hostname, term vector> 键值对

![image-20220321190256065](../../resources/map_reduce/map_reduce_01.png)

 **倒排索引（Inverted Index）**： map 函数解析每个文档， 然后以<word, document ID> （单词， 文档ID）这种形式输出。reduce 函数接收这串数据， 然后根据每个word，对相关得文档ID进行排序， 然后以<word, list(document ID)> 这种形式输出。 这种输出的键值对形成了一个简单的倒置索引， 这样方便我们去高效地去记录每个单词出现的位置

**分布式排序（Distributed Sort）**：map 函数将每个记录中的key进行抽取， 然后输出<key， record> 键值对，然后reduce 函数将这些键值对原封不动的进行输出。这个计算依赖于分区的设备（4.1 章节描述）还有排序配置（4.2描述）

## 3. 实现（Implementation）

MapReduce 接口的实现有许多， 我们需要因地制宜。例如， 一种实现可能适合在一台小的共享内存的机器上面运行，另外一种实现可能更适合一个在多处理器NUMA架构下运行， 或者还有一种适合在由大量网络互联的机器集群上面运行。

这个章节描述了一种在Google计算环境广泛运行的MapReduce的实现， 这种计算环境：由大量以太网互联的普通PC机器构成的集群。环境构成如下：

1. 机器一般是Linux机器，每台机器 2-4GB的内存， 双核x86的处理器
2. 网卡设备一般是100M/s的速度，或者是1G/s (机器级别)，但是平均下来一半都用不了
3. 一个集群由成千上百台机器组成，机器故障是常有的事
4. 每台机器上面挂载着普通的磁盘。我们开发了分布式文件系统， 在这些磁盘上管理数据。 这个文件系统使用复制手段， 在这些不可靠的机器上面提供可用性和可靠性
5. 用户提交Job(作业)到一个调度系统。每个Job由一组task组成， 并且被调度系统分配到集群上面的一组可用的机器上面。

### 3.1 运行概览（Execution Overview）

通过将Map调用的输入数据自动分割为 *M* 个数据片段的集合， Map 调用被分布到多台机器上执行。输入的数据片段能够在不同的机器上面并行处理。Reduce 函数将Map调用产生的中间状态的 key 值分成 *R* 个不同分区（例如，hash(key)* **mod** *R*）。Reduce 函数也被分不到多台机器上执行。分区数量 *R* 和分区函数由用户定义

图1（Figure 1）展示了一种MapReduce的运行方式。当用户变成调用MapReduce 函数的时候，会按照下面的时序执行（执行序号与图1对应）

1. 首先，用户程序中的MapReduce 会将输入的文件拆分成为 *M* 片段 （一般16-64MB大小，用户可以自定义大小）。然后每个集群上面的机器程序副本会开始对数据开始处理
2. 有一种程序副本比较特殊-master。 副本中的其他程序是 workers，由master程序负责分发工作。现在有*M*个map任务还有*R* 个reduce任务去分配， master 会挑选空闲的workers去分配一个map任务或者是reduce任务
3. 
